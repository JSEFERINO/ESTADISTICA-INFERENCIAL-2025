{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp6Mni6iUR+4OtHTyyc+B6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSEFERINO/ESTADISTICA-INFERENCIAL-2025/blob/main/D_Estimaci%C3%B3n_de_par%C3%A1metros_poblacionales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Curso de Estadística: Parte III: Inferencial - En construcción**\n",
        "\n",
        "**Autor:** Julio Hurtado Marquez\n",
        "**Correo Electrónico:** juliohurtado210307@gmail.com\n",
        "**Fecha:** Año 2025"
      ],
      "metadata": {
        "id": "mzX-wZ0aAeTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Librerias para Usar R**"
      ],
      "metadata": {
        "id": "r53CzCzG99Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar R y rpy2\n",
        "!apt-get install -y r-base\n",
        "!pip install rpy2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H3unjfnePEua",
        "outputId": "330d4f2e-3705-463b-8d4f-ed9fadae8298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "r-base is already the newest version (4.5.0-2.2204.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.11/dist-packages (3.5.17)\n",
            "Requirement already satisfied: cffi>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from rpy2) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from rpy2) (3.1.6)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from rpy2) (5.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.15.1->rpy2) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->rpy2) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar rpy2 para ejecutar código en R\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "746zB7JEPOTv",
        "outputId": "a99ad3e7-23ad-401a-aee8-cc60093aa966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The rpy2.ipython extension is already loaded. To reload it, use:\n",
            "  %reload_ext rpy2.ipython\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **D. Estimación de parámetros poblacionales**\n",
        "\n",
        "## **11. Estimación de parámetros poblacionales**\n",
        "\n",
        "### **11.1. Introducción**\n",
        "\n",
        "+ **El objetivo de la estadística es obtener una inferencia con respecto a la población basándose en la información contenida en una muestra. Como las poblaciones se describen mediante medidas numéricas denominadas **parámetros**, la mayoría de las investigaciones se conducen en deducir inferencias acerca de ellos. Los procedimientos de la inferencia estadística involucran ya sea la _estimación__ o la _Prueba de Hipótesis_, las cuales tienen muchas aplicaciones prácticas.**\n",
        "\n",
        "---\n",
        "\n",
        "### **11.2. Estimadores Puntuales y sus Propiedades**\n",
        "\n",
        "+ *Básicamente, para que un estimador sea bueno, se desea que la varianza del estimador sea lo más pequeña posible, mientras que la distribución de muestreo debe concentrarse alrededor del valor del parámetro*.\n",
        "\n",
        "### **Definición**\n",
        "\n",
        "+ Un estimador $\\hat{\\theta}$ de un parámetro poblacional $\\theta$ es una regla que establece cómo calcular una estimación del parámetro basada en las mediciones contenidas en una muestra aleatoria. Comúnmente, el estimador se expresa mediante una fórmula. Por ejemplo, la media de la muestra:\n",
        "\n",
        "$$\n",
        "\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i\n",
        "$$\n",
        "\n",
        "+ es un posible estimador puntual para la media poblacional $\\mu$. Es evidente que $\\bar{Y}$ es una regla y una fórmula al mismo tiempo.\n",
        "\n",
        "---\n",
        "\n",
        "### **11.3. Estimadores Insesgados**\n",
        "\n",
        "Se dice que la estadística $\\hat{\\theta} = H(X_1, X_2, \\dots, X_n)$ es un **estimador insesgado** del parámetro $\\theta$ si:\n",
        "\n",
        "$$\n",
        "E(\\hat{\\theta}) = \\theta\n",
        "$$\n",
        "\n",
        "Es decir, si los valores del estimador se centran alrededor del parámetro en cuestión. En caso contrario, se dice que es **sesgado**.\n",
        "\n",
        "---\n",
        "\n",
        "### **11.4. Estimadores Insesgados Comunes**\n",
        "\n",
        "En inferencia estadística, los estimadores puntuales insesgados más utilizados son:\n",
        "\n",
        "| Parámetro objetivo: $\\theta$ | Tamaño | Estimador: $\\hat{\\theta}$ | $E(\\hat{\\theta})$ | $V(\\hat{\\theta})$ |\n",
        "|------------------------------|--------|---------------------------|-------------------|-------------------|\n",
        "| $\\mu$                        | $n$    | $\\bar{Y}$                 | $\\mu$             | $\\frac{\\sigma^2}{n}$ |\n",
        "| $p$                          | $n$    | $\\hat{p} = \\frac{Y}{n}$   | $p$               | $\\frac{pq}{n}$    |\n",
        "| $\\mu_1 - \\mu_2$              | $n_1$ y $n_2$ | $\\bar{Y}_1 - \\bar{Y}_2$ | $\\mu_1 - \\mu_2$   | $\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$ |\n",
        "| $p_1 - p_2$                  | $n_1$ y $n_2$ | $\\hat{p}_1 - \\hat{p}_2$ | $p_1 - p_2$       | $\\frac{p_1 q_1}{n_1} + \\frac{p_2 q_2}{n_2}$ |\n",
        "\n",
        "---\n",
        "\n",
        "### **11.5. Ejemplo: Estimadores Insesgados y Sesgados**\n",
        "\n",
        "Sea $Y_1, Y_2, \\dots, Y_n$ una muestra aleatoria con $E(Y_i) = \\mu$ y $V(Y_i) = \\sigma^2$. Demuestre que:\n",
        "\n",
        "1. $S^{*2} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ es un estimador **sesgado** para $\\sigma^2$.\n",
        "2. $S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\bar{Y})^2$ es un estimador **insesgado** para $\\sigma^2$.\n",
        "\n",
        "**Solución:**\n",
        "\n",
        "1. **Paso 1:** Expresamos $(Y_i - \\bar{Y})^2$ como:\n",
        "   $$\n",
        "   (Y_i - \\bar{Y})^2 = Y_i^2 - 2Y_i \\bar{Y} + \\bar{Y}^2\n",
        "   $$\n",
        "\n",
        "2. **Paso 2:** Sumamos sobre todas las observaciones:\n",
        "   $$\n",
        "   \\sum_{i=1}^n (Y_i - \\bar{Y})^2 = \\sum_{i=1}^n Y_i^2 - 2n \\bar{Y}^2 + n \\bar{Y}^2 = \\sum_{i=1}^n Y_i^2 - n \\bar{Y}^2\n",
        "   $$\n",
        "\n",
        "3. **Paso 3:** Calculamos la esperanza:\n",
        "   $$\n",
        "   E\\left( \\sum_{i=1}^n (Y_i - \\bar{Y})^2 \\right) = E\\left( \\sum_{i=1}^n Y_i^2 - n \\bar{Y}^2 \\right) = \\sum_{i=1}^n E(Y_i^2) - n E(\\bar{Y}^2)\n",
        "   $$\n",
        "\n",
        "4. **Paso 4:** Recordamos que:\n",
        "   $$\n",
        "   E(Y_i^2) = \\sigma^2 + \\mu^2 \\quad \\text{y} \\quad E(\\bar{Y}^2) = \\frac{\\sigma^2}{n} + \\mu^2\n",
        "   $$\n",
        "\n",
        "5. **Paso 5:** Sustituimos:\n",
        "   $$\n",
        "   E\\left( \\sum_{i=1}^n (Y_i - \\bar{Y})^2 \\right) = n(\\sigma^2 + \\mu^2) - n\\left( \\frac{\\sigma^2}{n} + \\mu^2 \\right) = (n-1)\\sigma^2\n",
        "   $$\n",
        "\n",
        "6. **Paso 6:** Por tanto:\n",
        "   $$\n",
        "   E(S^{*2}) = \\frac{1}{n} E\\left( \\sum_{i=1}^n (Y_i - \\bar{Y})^2 \\right) = \\frac{(n-1)\\sigma^2}{n}\n",
        "   $$\n",
        "   Esto muestra que $S^{*2}$ es sesgado.\n",
        "\n",
        "7. **Paso 7:** Para $S^2$:\n",
        "   $$\n",
        "   E(S^2) = \\frac{1}{n-1} E\\left( \\sum_{i=1}^n (Y_i - \\bar{Y})^2 \\right) = \\sigma^2\n",
        "   $$\n",
        "   Por tanto, $S^2$ es insesgado.\n",
        "\n",
        "---\n",
        "\n",
        "### **11.6. Estimadores Consistentes**\n",
        "\n",
        "+ Es razonable esperar que un buen estimador de un parámetro $\\theta$ sea cada vez mejor conforme crece el tamaño de la muestra y la información se vuelve más completa. La distribución de muestreo de un buen estimador se encuentra cada vez más concentrada alrededor del parámetro $\\theta$. Si un estimador es **consistente**, converge en probabilidad al valor del parámetro que está intentando estimar conforme el tamaño de la muestra crece. Esto implica que la varianza de un estimador consistente disminuye conforme $n$ crece.\n",
        "\n",
        "+ Se dice que $\\hat{\\theta}$ es un estimador consistente de $\\theta$ si:\n",
        "\n",
        "$$\n",
        "\\lim_{n \\to \\infty} \\hat{\\theta} = \\theta\n",
        "$$\n",
        "\n",
        "+ que es equivalente a:\n",
        "\n",
        "$$\n",
        "\\lim_{n \\to \\infty} V(\\hat{\\theta}) = 0\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **11.7.  Estimadores Eficientes (Insesgados de Varianza Mínima)**\n",
        "\n",
        "+ El hecho de que un estimador sea centrado no garantiza que sus realizaciones caigan cerca del valor del parámetro; hace falta además que tenga la varianza pequeña. La varianza de un estimador insesgado es la cantidad más importante para decidir qué tan bueno es el estimador para estimar el parámetro $\\theta$.\n",
        "\n",
        "+ Sean $\\hat{\\theta}_1$ y $\\hat{\\theta}_2$ cualesquiera dos estimadores insesgados de $\\theta$. Se dice que $\\hat{\\theta}_1$ es un estimador más eficiente de $\\theta$ que $\\hat{\\theta}_2$ si:\n",
        "\n",
        "$$\n",
        "V(\\hat{\\theta}_1) \\leq V(\\hat{\\theta}_2)\n",
        "$$\n",
        "\n",
        "+ El cociente:\n",
        "\n",
        "$$\n",
        "e = \\frac{V(\\hat{\\theta}_1)}{V(\\hat{\\theta}_2)}\n",
        "$$\n",
        "\n",
        "+ se llama **eficiencia relativa** de $\\hat{\\theta}_1$ respecto a $\\hat{\\theta}_2$, y su valor está entre 0 y 1 ($0 \\leq e \\leq 1$). Si $e$ está próximo a 0, $\\hat{\\theta}_1$ es mejor que $\\hat{\\theta}_2$.\n",
        "\n",
        "---\n",
        "\n",
        "### **11.8. Bondad de un Estimador**\n",
        "\n",
        "+ El **error de estimación** $\\epsilon$ es la distancia entre un estimador y su parámetro objetivo. Es decir:\n",
        "\n",
        "$$\n",
        "\\epsilon = |\\hat{\\theta} - \\theta|\n",
        "$$\n",
        "\n",
        "+ Ya que el error de estimación es una cantidad aleatoria, no podemos afirmar qué tan grande o pequeño será para una estimación en particular, pero se pueden establecer enunciados probabilísticos al respecto. Si se conoce la distribución de probabilidad de $\\hat{\\theta}$, se pueden elegir dos puntos $(\\theta - b)$ y $(\\theta + b)$ localizados cerca de las colas de la distribución de manera que:\n",
        "\n",
        "$$\n",
        "P(\\epsilon < b) = P(\\theta - b < \\hat{\\theta} < \\theta + b)\n",
        "$$\n",
        "\n",
        "+ y $b$ se puede considerar como el límite probabilístico del error de estimación con una alta probabilidad.\n",
        "\n",
        "---\n",
        "\n",
        "### **11.9. Ejemplo: Estimación de una Proporción**\n",
        "\n",
        "+ Una muestra de $n = 1000$ votantes, obtenida al azar de una ciudad, mostró $y = 560$ a favor del candidato Gómez. Estime $p$, la fracción de votantes en la población que están a favor de Gómez, y utilice un límite de dos desviaciones estándar para el error de estimación.\n",
        "\n",
        "**Solución:**\n",
        "\n",
        "1. Utilizamos el estimador $\\hat{p} = \\frac{Y}{n}$ para estimar $p$. Así, la estimación de $p$ es:\n",
        "\n",
        "   $$\n",
        "   \\hat{p} = \\frac{560}{1000} = 0.56\n",
        "   $$\n",
        "\n",
        "2. La distribución de probabilidad de $\\hat{p}$ se aproxima con bastante exactitud mediante la distribución normal para muestras tan grandes como $n = 1000$. Entonces, cuando $b = 2\\sigma_{\\hat{p}}$, se tiene que $P(\\epsilon < b) \\approx 0.95$.\n",
        "\n",
        "3. Calculamos $b$:\n",
        "\n",
        "   $$\n",
        "   b = 2\\sigma_{\\hat{p}} = 2\\sqrt{\\frac{pq}{n}}\n",
        "   $$\n",
        "\n",
        "   Como no conocemos $p$, usamos $\\hat{p}$ para aproximar:\n",
        "\n",
        "   $$\n",
        "   b \\approx 2\\sqrt{\\frac{(0.56)(0.44)}{1000}} = 0.03\n",
        "   $$\n",
        "\n",
        "4. Este resultado significa que la probabilidad de que el error de estimación sea menor que 0.03 es aproximadamente 0.95.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Sca6ZlsftP1Q"
      }
    }
  ]
}